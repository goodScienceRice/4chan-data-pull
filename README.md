# 4chan Scraping and Tokenizing Project   

This project is a set of tools for scraping 4chan posts from specific boards, merging the scraped text data, and then tokenizing the text data for further processing. The project includes scripts for scraping both 4chan and 4plebs archive pages, merging text files, and tokenizing posts using a pre-trained transformer model.

## Project Structure

- `alt-4chan-formsubmit.py`: Scrapes posts from a live 4chan board using infinite scroll. Saves each thread's posts in a separate text file.
- `alt-archive-4plebs.py`: Scrapes posts from the 4plebs archive, saves each thread's posts in a text file.
- `merge.py`: Merges multiple `.txt` files into a single output file.
- `tokenizer.py`: Tokenizes the merged text data using a transformer model and can output the results in JSON or CSV format.
- `requirements.txt`: Contains the list of dependencies required for the project.

## Installation

## 1. Clone this repository:
```bash
git clone https://github.com/goodScienceRice/4chan-data-pull.git
   
cd 4chan-data-pull
``` 

## Install the required Python packages using pip3:
```bash
pip3 install -r requirements.txt
```
## 2. Install playwright browsers for scraping:
```bash
playwright install
```
## Usage
### 1. Scrape from 4chan
Use the alt-4chan-formsubmit.py script to scrape live posts from a 4chan board. The script automatically scrolls through pages and extracts thread content.

### Example usage:
```bash
python3 alt-4chan-formsubmit.py -b [board_name] -s [max_scrolls] -o [output_directory]
```
-b: The name of the board to scrape (default is pol).
-s: Maximum number of scrolls per page (default is 10).
-o: The directory to save the scraped threads (default is output).

This script will save each thread's posts into a separate text file in the specified directory.

## 2. Scrape from 4plebs Archive
Use the alt-archive-4plebs.py script to scrape posts from the 4plebs archive.

### Example usage:
```bash
python3 alt-archive-4plebs.py -b [board_name] -p [page_number]
```
-b: The board to scrape (e.g., pol).
-p: The starting page number (default is 1).

This script will navigate through the 4plebs archive, saving each thread's posts into separate .txt files.

##  3. Merge Text Files
Use the merge.py script to combine the text files generated by the scraping scripts into a single file.

### Example usage:
```bash
python3 merge.py -d [input_directory] -o [output_file]
```
-d: The directory containing the .txt files to merge.
-o: The path to the output file.

## 4. Tokenize the Merged Data
Use the tokenizer.py script to tokenize the merged text file using a pre-trained transformer model.

### Example usage:
```bash
python3 tokenizer.py [input_file] [output_file]
```
- input_file: The path to the merged .txt file.
- output_file: The path where the tokenized data will be saved (supports .json or .csv format).

The script tokenizes each post's text and stores the results, including token length and tokens, in the output file.

## Example Workflow

### 1. Scrape data from 4chan:
```bash
python3 alt-4chan-formsubmit.py -b pol -s 10 -o scraped_data
```
### 2. Merge the scraped .txt files:
```bash
python3 merge.py -d scraped_data -o merged_data.txt
```
### 3. Tokenize the merged data:
```bash
python3 tokenizer.py merged_data.txt tokenized_data.json
```
## Dependencies
This project requires the following libraries (listed in requirements.txt):

- argparse
- transformers
- playwright